{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradeeptandon/Pradeep_assignment/blob/master/NLP_analysis_of_PDF_documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'manifesto:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F413327%2F791006%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240719%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240719T082449Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6d3e1e06bd571314c942187cc5047c396a2e27f65dc257044bb3be80dba5de27f690962fe5419fd6642407ab2e7d03eabefd5d4e0033a32ca7c1c54f57145c9f193f8ddfb94d238115827b23f0a811a306433782bcc73031a880ae9aa12063b385c203c9bcfd2c19e669816912716166857ddc5d40fe9349f4c35a542e977bad9021abe449ec3c24cba0d00adf18f72a4cd454aae7bd3716555da7289812953cfe7d6d88ba054563a888fa740a8d8c93e583ec0b7bdf8e9f1a7dbf49de5f60eaa38aee26622e99edf4d8c0e74bd0d5f6a5c924db7b59c6cd32f90908f1d52399852a77b20264b1c42d08deb4a4b9dab9147b9b217096690bb91c6f89bec3b5c7'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ujWLV6u8GBV-",
        "outputId": "659f372b-ca6f-49a0-a5b8-8e706f37ec0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading manifesto, 14047185 bytes compressed\n",
            "[==================================================] 14047185 bytes downloaded\n",
            "Downloaded and uncompressed: manifesto\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "YFT5kbPYGBWB"
      },
      "cell_type": "markdown",
      "source": [
        "# **Analysis of Independent Presidential Candidata Dr. Iitula's Manifesto for 2019  Election Campaign**"
      ]
    },
    {
      "metadata": {
        "id": "QzS1m1LEGBWC"
      },
      "cell_type": "markdown",
      "source": [
        "### **1. Imports**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-7Tqkw_iGBWC",
        "outputId": "e22bed9b-12cd-43ef-a026-83ced92ff1f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pO2kDpnOGBWC"
      },
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0DQKoyAtGBWD"
      },
      "cell_type": "markdown",
      "source": [
        "### **2. Read our document**"
      ]
    },
    {
      "metadata": {
        "id": "3QCtRl0AGBWD"
      },
      "cell_type": "markdown",
      "source": [
        "Lets read our pdf for the manifesto using the `PdfFileReader()` function from the PyPDF2 which is a package for extracting document information such as **title, author, number of pages,....**, spliting documents page by page, merging page by page, etc."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "u6fYHSj7GBWD"
      },
      "cell_type": "code",
      "source": [
        "filename = '../input/manifesto/manibook'\n",
        "open_filename = open(filename, 'rb')\n",
        "\n",
        "ind_manifesto = PyPDF2.PdfReader(open_filename)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bHNDcIZYGBWE"
      },
      "cell_type": "markdown",
      "source": [
        "To get the document informtion  ussing the `getDocumentInfo()` function and check the number of pages in our document using the `numPages()` function. There are various useful functions one can use to check other things. See online documentation:[PyPDF2](https://pythonhosted.org/PyPDF2/index.html)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XW8zc161GBWE",
        "outputId": "77a7af30-fb27-4f1a-ac39-8b5bc8eb59df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "ind_manifesto.metadata"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/Title': 'Microsoft Word - MANIBOOK  A4 L.docx',\n",
              " '/Producer': 'macOS Version 10.14.3 (Build 18D109) Quartz PDFContext',\n",
              " '/Creator': 'Word',\n",
              " '/CreationDate': \"D:20191025172054Z00'00'\",\n",
              " '/ModDate': \"D:20191025172054Z00'00'\",\n",
              " '/Keywords': '',\n",
              " '/AAPL:Keywords': []}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0t_lGhSlKoxK",
        "outputId": "fa911a00-5210-4662-eb9b-60f5aca97e4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "5GhgvfsNGBWE",
        "outputId": "661c1ef8-69b1-4fdd-a044-24ff15aa600e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "total_pages = len(ind_manifesto.pages)\n",
        "total_pages"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "DgxezUJsGBWE"
      },
      "cell_type": "markdown",
      "source": [
        "From the outputs of our two previous codes, we got the **title** of the document, what OS was used to type the document, when the document was created and modified. And we also got the total number of pages in our document."
      ]
    },
    {
      "metadata": {
        "id": "JLDCsxy3GBWE"
      },
      "cell_type": "markdown",
      "source": [
        "### **3. Lets extract the texts from the pdf file and print it**\n",
        "\n",
        "We will use a `textract` package to extract our texts from the document."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CRfxcAnOGBWF",
        "outputId": "aa19e4e0-51f5-48e4-d7d5-e7d112d79d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install textract\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting argcomplete~=1.10.0 (from textract)\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting beautifulsoup4~=4.8.0 (from textract)\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==3.* (from textract)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docx2txt~=0.8 (from textract)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting extract-msg<=0.29.* (from textract)\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20191110 (from textract)\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-pptx~=0.6.18 (from textract)\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six~=1.12.0 (from textract)\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting SpeechRecognition~=3.8.1 (from textract)\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xlrd~=1.2.0 (from textract)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodome (from pdfminer.six==20191110->textract)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.5)\n",
            "Collecting imapclient==2.1.0 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting olefile>=0.46 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tzlocal>=2.1 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (5.2)\n",
            "Collecting compressed-rtf>=1.0.6 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ebcdic>=1.1.1 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx~=0.6.18->textract)\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=1386af558c1e2b03a9f77cd9234a89d7ae6d76f5f793c9480f61fc2de6eb0024\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6185 sha256=d849b7ea0ad14b32252b4393f950372c3f383f5c017418df1ee59aa4859c85d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/3e/48/e7d833ecc516c36f8966d310b1a6386db091a718f1ff3bf85c\n",
            "Successfully built docx2txt compressed-rtf\n",
            "Installing collected packages: SpeechRecognition, ebcdic, docx2txt, compressed-rtf, chardet, argcomplete, XlsxWriter, xlrd, six, pycryptodome, olefile, beautifulsoup4, python-pptx, pdfminer.six, imapclient, extract-msg, textract\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.12.3\n",
            "    Uninstalling beautifulsoup4-4.12.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.12.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.6.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "yfinance 0.2.40 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SpeechRecognition-3.8.1 XlsxWriter-3.2.0 argcomplete-1.10.3 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.47 pdfminer.six-20191110 pycryptodome-3.20.0 python-pptx-0.6.23 six-1.12.0 textract-1.6.5 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              },
              "id": "6860b40166814108b931dc760249cd13"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nphDSM28GBWF"
      },
      "cell_type": "code",
      "source": [
        "import textract"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gItBK5_VGBWF"
      },
      "cell_type": "markdown",
      "source": [
        "Loop throug all the pages in the document and extract the text from it"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "VjlDdgMoGBWF"
      },
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "text  = ''\n",
        "\n",
        "# Lets loop through, to read each page from the pdf file\n",
        "while(count < total_pages):\n",
        "    # Get the specified number of pages in the document\n",
        "    mani_page  = ind_manifesto.pages[count]\n",
        "    # Process the next page\n",
        "    count += 1\n",
        "    # Extract the text from the page\n",
        "    text += mani_page.extract_text()\n",
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ed2s_02cGBWF"
      },
      "cell_type": "markdown",
      "source": [
        "The `if` statement check if our document returned words from the loop above using the `extractText()` function. This is done since `PyPDF2` cannot read scanned documents."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "erFaAGxQGBWF"
      },
      "cell_type": "code",
      "source": [
        "if text != '':\n",
        "    text = text\n",
        "\n",
        "else:\n",
        "    textract.process(open_filename, method='tesseract', encoding='utf-8', langauge='eng' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8e-jyYSvGBWG"
      },
      "cell_type": "markdown",
      "source": [
        "If the above returns a false, then run the Optical Character Recognition (OCR) `textract` to convert scanned/image based Pdf files to text. See `textract` online documentaion: [textract](https://textract.readthedocs.io/en/stable/python_package.html)."
      ]
    },
    {
      "metadata": {
        "id": "4Hnyfc_RGBWG"
      },
      "cell_type": "markdown",
      "source": [
        "Lets print out our texts to see what it contains which was converted to lower case using the `lower()` function."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fp7L0lEAGBWG"
      },
      "cell_type": "code",
      "source": [
        "!pip install autocorrect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "2teZojNSGBWG"
      },
      "cell_type": "code",
      "source": [
        "from autocorrect import Speller\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "def to_lower(text):\n",
        "\n",
        "    \"\"\"\n",
        "    Converting text to lower case as in, converting \"Hello\" to  \"hello\" or \"HELLO\" to \"hello\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Specll check the words\n",
        "    spell  = Speller(lang='en')\n",
        "\n",
        "    texts = spell(text)\n",
        "\n",
        "    return ' '.join([w.lower() for w in word_tokenize(text)])\n",
        "\n",
        "lower_case = to_lower(text)\n",
        "print(lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZdwJhkYGBWG"
      },
      "cell_type": "markdown",
      "source": [
        "And clear from seeing the printed text, we only extracted texts from page 2 till last pace. The reason being is that, those pages that we did not extract text from are in image based and we failed to to do. **SOMEONE CAN PERHAPS HELP!!!!.**"
      ]
    },
    {
      "metadata": {
        "id": "7d7JSJTzGBWG"
      },
      "cell_type": "markdown",
      "source": [
        "### **4. Clean our *to_lower_case* text variable and return it as a list of keywords.**\n",
        "\n",
        "From the printed text, it's apparent that our text contains unwanted characters such as spaces, punctuations `\\n` and so forth.\n",
        "\n",
        "Lets break our text phrases into individual words using `word_tokenize()` function from the Naturalge Toolkit (nltk)."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "jeyR-p1-GBWG"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords, brown\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from autocorrect import spell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "S7fotD93GBWH"
      },
      "cell_type": "code",
      "source": [
        "def clean_text(lower_case):\n",
        "    # split text phrases into words\n",
        "    words  = nltk.word_tokenize(lower_case)\n",
        "\n",
        "\n",
        "    # Create a list of all the punctuations we wish to remove\n",
        "    punctuations = ['.', ',', '/', '!', '?', ';', ':', '(',')', '[',']', '-', '_', '%']\n",
        "\n",
        "    # Remove all the special characters\n",
        "    punctuations = re.sub(r'\\W', ' ', str(lower_case))\n",
        "\n",
        "    # Initialize the stopwords variable, which is a list of words ('and', 'the', 'i', 'yourself', 'is') that do not hold much values as key words\n",
        "    stop_words  = stopwords.words('english')\n",
        "\n",
        "    # Getting rid of all the words that contain numbers in them\n",
        "    w_num = re.sub('\\w*\\d\\w*', '', lower_case).strip()\n",
        "\n",
        "    # remove all single characters\n",
        "    lower_case = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', lower_case)\n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    lower_case = re.sub(r'\\s+', ' ', lower_case, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
        "\n",
        "\n",
        "\n",
        "    # Removing non-english characters\n",
        "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
        "\n",
        "    # Return keywords which are not in stop words\n",
        "    keywords = [word for word in words if not word in stop_words  and word in punctuations and  word in w_num]\n",
        "\n",
        "    return keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "QM3zbyGYGBWH"
      },
      "cell_type": "code",
      "source": [
        "# Lemmatize the words\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in clean_text(lower_case)]\n",
        "\n",
        "# lets print out the output from our function above and see how the data looks like\n",
        "clean_data = ' '.join(lemmatized_word)\n",
        "print(clean_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "dhAxqM98GBWH"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XqQxzSosGBWH"
      },
      "cell_type": "markdown",
      "source": [
        "Lets save our data into a dataframe so we can do our anal"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "EqYpjib9GBWH"
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([clean_data])\n",
        "df.columns = ['script']\n",
        "df.index = ['Itula']\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mZ9X3ZcEGBWI"
      },
      "cell_type": "markdown",
      "source": [
        "###  **5. Preprocess - Bag of Words model**\n",
        "\n",
        "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision [Wikipedia](https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/).\n",
        "\n",
        "\n",
        "It is mostly used to extract features from text for used in modelling, such as machine learning algorithms."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "d-zfiQkQGBWI"
      },
      "cell_type": "code",
      "source": [
        "#  Counting the occurrences of tokens and building a sparse matrix of documents x tokens.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "corpus = df.script\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Transforms the data into a bag of words\n",
        "data_vect = vect.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aa9FYmAGGBWI"
      },
      "cell_type": "markdown",
      "source": [
        "Applt the document-term matrix which is a mathematical matrix which decribes the frequency of words in a document"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "9Arzrv3TGBWI"
      },
      "cell_type": "code",
      "source": [
        "feature_names = vect.get_feature_names()\n",
        "data_vect_feat = pd.DataFrame(data_vect.toarray(), columns=feature_names)\n",
        "data_vect_feat.index = df.index\n",
        "data_vect_feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9OU5fYqvGBWI"
      },
      "cell_type": "markdown",
      "source": [
        "Our vector representations show the frequency of words used in the document"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2Owx6pIoGBWI"
      },
      "cell_type": "code",
      "source": [
        "data = data_vect_feat.transpose()\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZEwuiVfuGBWI"
      },
      "cell_type": "markdown",
      "source": [
        "### **6. Getting the top 100 frequent words from the manifesto.**\n",
        "\n",
        "We will try to get the top most common 100 words from our document and plot that into a wordcloud for visualization."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "W_YBf2UVGBWI"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "# Find the top 1000 words written in the manifesto\n",
        "top_dict = {}\n",
        "for c in data.columns:\n",
        "    top = data[c].sort_values(ascending=False)\n",
        "    top_dict[c]= list(zip(top.index, top.values))\n",
        "\n",
        "\n",
        "for x in list(top_dict)[0:100]:\n",
        "    print(\"key {}, value {} \".format(x,  top_dict[x]))\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "jJEwYp3PGBWN"
      },
      "cell_type": "code",
      "source": [
        "# Look at the most common top words --> add them to the stop word list\n",
        "from collections import Counter\n",
        "\n",
        "# Let's first pull out the top 100 words for each comedian\n",
        "words = []\n",
        "for president in data:\n",
        "    top = [word for (word, count) in top_dict[president]]\n",
        "    for t in top:\n",
        "        words.append(t)\n",
        "\n",
        "print(words[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yi_czhGFGBWN"
      },
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "# Image used in which our world cloud output will be\n",
        "img1 = imageio.imread(\"../input/manifesto/itula.jpeg\")\n",
        "hcmask1 = img1\n",
        "\n",
        "# Get 100 words based on the\n",
        "words_except_stop_dist = nltk.FreqDist(w for w in words[:100])\n",
        "wordcloud = WordCloud(stopwords=set(STOPWORDS),background_color='black',mask=hcmask1).generate(\" \".join(words_except_stop_dist))\n",
        "plt.imshow(wordcloud,interpolation = 'bilinear')\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(10,12)\n",
        "plt.axis('off')\n",
        "plt.title(\"Top most common 100 words from Dr. Itula's Manifesto 2019\",fontsize=20)\n",
        "plt.tight_layout(pad=0)\n",
        "plt.savefig('Manifesto_top_100.jpeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6nNC75YtGBWN"
      },
      "cell_type": "markdown",
      "source": [
        "### **7. Sentiment Analysis of the Manifesto**\n",
        "\n",
        "This is a set of Natural Language Processing (NLP) technique of analysing, identifying and categorizing opinions expressed in a piece of text, in order to determine whether the writer's attitude towards a particular topic, product, politics, services, brands etc. is positive, negative, or neutral. This data holds immense value in the fields of marketing analysis, public relations, product reviews, net promoter scoring, product feedback, and customer service, for example."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "B78xTBw5GBWN"
      },
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9NcK_547GBWN"
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5kavEwdDGBWN"
      },
      "cell_type": "markdown",
      "source": [
        "Lets decide  which model we should use, between **TextBlob** and **VADER** for analysis of our text. We will therefore use TextBlob for its simplcity, and since VADER is specifically for analysis of social media data.  \n",
        "\n",
        "#### **7.1 TextBlob function - returns two properties**\n",
        "\n",
        "**Polarity:** a float value which ranges from [-1.0 to 1.0] where 0 indicates neutral, +1 indicates most positive statement and -1 rindicates  most negative statement.\n",
        "\n",
        "**Subjectivity:** a float value which ranges from [0.0 to 1.0] where 0.0 is most objective while 1.0 is most subjective. Subjective sentence expresses some personal opinios, views, beliefs, emotions, allegations, desires, beliefs, suspicions, and speculations where as objective refers to factual information."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XQall4L1GBWN"
      },
      "cell_type": "code",
      "source": [
        "blob = TextBlob(clean_data)\n",
        "blob.sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7N1tHHHBGBWO"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that the polarity is **0.07** which means that the document is **neutral** and **0.47** subjectivity refers almost factual information in the document rather than public opinions, beliefs and so forth."
      ]
    },
    {
      "metadata": {
        "id": "Mf1LbDQTGBWO"
      },
      "cell_type": "markdown",
      "source": [
        "## **8. Text Summarization of the Document**\n",
        "\n",
        "![title](https://miro.medium.com/max/1200/1*GIVviyN9Q0cqObcy-q-juQ.png)\n",
        "\n",
        "\n",
        "Text summarization is an important NLP task which is a way of producing a concise and fluent summary of a perticular textin an article, journal, book, comment review, etc while also preseving the key information and overall meaning. Its is divided into categories, i.e., **extraction** and **abstraction**. Extractive methods select a subset of existing words, phrases, or sentences in the original text to form a summary. In contrast, abstractive methods first build an internal semantic representation and then use natural language generation techniques to create a summary.\n",
        "\n",
        "\n",
        "Build a similarity matrix $\\rightarrow$  generate rank based on matrix  $\\rightarrow$ pick top N sentences for summary."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XQl6NzTjGBWO"
      },
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-3m-ObhdGBWO"
      },
      "cell_type": "markdown",
      "source": [
        "Create a vectors between two sntences and calculate the cosine angel between them."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zBWrwRidGBWO"
      },
      "cell_type": "code",
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        "\n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "# One out of 5 words differ => 0.8 similarity\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split()))\n",
        "\n",
        "# One out of 2 non-stop words differ => 0.5 similarity\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split(), stopwords.words('english')))\n",
        "\n",
        "# 0 out of 2 non-stop words differ => 1 similarity (identical sentences)\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a good sentence\".split(), stopwords.words('english')))\n",
        "\n",
        "# Completely different sentences=> 0.0\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"I want to go to the market\".split(), stopwords.words('english')))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wOFnDRXKGBWO"
      },
      "cell_type": "markdown",
      "source": [
        "We will use **unsupervised machine learning** approach to find the sentences similarity and rank them using the **cosine similarity** approach. Cosine simirality measures and calculates the similarity between two non-zero vectors of an inner product space by measuring the cosine angle between them.  One benefit of the **cosine similarity** approach is, there;s no need to train and build a model prior start using it for your project."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pmVyGCR7GBWO"
      },
      "cell_type": "code",
      "source": [
        "#print(sentences)\n",
        "\n",
        "# get the english list of stopwords\n",
        "#stop_words = stopwords.words('english')\n",
        "\n",
        "def build_similarity_matrix(lower_case, stopwords=None):\n",
        "    # Create an empty similarity matrix\n",
        "    S = np.zeros([len(lower_case), len(lower_case)])\n",
        "\n",
        "\n",
        "    for idx1 in range(len(lower_case)):\n",
        "        for idx2 in range(len(lower_case)):\n",
        "            if idx1 == idx2:\n",
        "                continue\n",
        "\n",
        "            S[idx1][idx2] = sentence_similarity(lower_case[idx1], lower_case[idx2], stop_words)\n",
        "\n",
        "    # normalize the matrix row-wise\n",
        "    for idx in range(len(S)):\n",
        "        S[idx] /= S[idx].sum()\n",
        "\n",
        "    return S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vymL-WOFGBWO"
      },
      "cell_type": "code",
      "source": [
        "#len(sentences)\n",
        "#S = build_similarity_matrix(sentences, stop_words)\n",
        "#S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MJxbpEr9GBWO"
      },
      "cell_type": "markdown",
      "source": [
        "Lets define a function to genrate our summary of the whole document. Also note we will be calling other helper function to keep our summarization pipeline going."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Mi5gtd9gGBWP"
      },
      "cell_type": "code",
      "source": [
        "def generate_summary(lower_case, top_n=5):\n",
        "    # Remove all the stopwords in the document\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "\n",
        "\n",
        "    #Read text and tokenize\n",
        "    #lower_case  = nltk.word_tokenize(lower_case)\n",
        "\n",
        "\n",
        "\n",
        "    #Generate similarity matrix across sentences\n",
        "    sentence_similarity  = build_similarity_matrix((lower_case, stop_words))\n",
        "\n",
        "    #Rank sentences in similarity matrix\n",
        "    sentence_similiraty_graph = nx.from_numpy_array(sentence_similarity)\n",
        "    scores = nx.pagerank(sentence_similiraty_graph)\n",
        "\n",
        "\n",
        "    #Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(lower_case)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(' '.join(ranked_sentence[i][1]))\n",
        "\n",
        "    #Output the summarized text\n",
        "    print('Summarized Text: \\n', '. '.join(summarize_text))\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "5fDD814XGBWP"
      },
      "cell_type": "code",
      "source": [
        "#generate_summary(lower_case, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AgLMz8tPGBWP"
      },
      "cell_type": "markdown",
      "source": [
        "Been trying to implement the tetxrank algorithm, however we keep getting an error whenever we call the `generate_summary()` function. Feel free to clarify on the error in order to generate a summary using this algorithm. With that said we found a simplest way to do it using just one line of code as shown in the following section."
      ]
    },
    {
      "metadata": {
        "id": "N57i00zNGBWP"
      },
      "cell_type": "markdown",
      "source": [
        "## **8.1 Gensim Package**\n",
        "\n",
        "\n",
        "Lets summarize our text sing the `gensim` package which is a  module for topic modelling for humans.  Summarizing is based on ranks of text sentences using a variation of the TextRank algorithm. see online documentation [gensim](https://radimrehurek.com/gensim/summarization/summariser.html)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "u_wsaPqcGBWP"
      },
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from gensim.summarization.summarizer import summarize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "v0aOy51DGBWP"
      },
      "cell_type": "code",
      "source": [
        "# Print out our summarized text of the document which was converted to lower case, remember we could have opted to remove stopwords as well.\n",
        "\n",
        "print(summarize(lower_case))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IyYxnFZpGBWP"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This was our summarized text from the document. Note: there are various summarization packages like `TextTeaser`, `PyTeaser` , etc which one can use to accomplish this with only one line of code."
      ]
    },
    {
      "metadata": {
        "id": "99wPz_uaGBWP"
      },
      "cell_type": "markdown",
      "source": [
        "## **9. Topic modeling using LDA**\n",
        "\n",
        "Topic modeling can be seen as a type of statistical modeling for discovering the abstract 'topics' that are presented in a myriad of documents (it can be a single document). A topic  is considered as a collection of prevalent keywords that are typical representatives. Its through keywords in which one determines what the topic is all about.\n",
        "\n",
        "### What is LDA?\n",
        "$\\rightarrow$ Latent Dirichlet Allocation(LDA) is a popular algorithm for topic modeling with excellent implementations in the Python’s `gensim`package. We will therefore use LDA  to classify text in our document to a particular topic. I works by building a topic per document model and words per topic model, from Dirichlet distribution models in statistics."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VR_g1X7oGBWQ"
      },
      "cell_type": "code",
      "source": [
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "#import graphlab as gl\n",
        "#import pyLDAvis.graphlab\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RUdTfpd8GBWQ"
      },
      "cell_type": "markdown",
      "source": [
        "Lets create a dictionary and corpus needed for topic modeling which are the two crucial inputs in implementint the LDA topc model"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "b86HeFvQGBWQ"
      },
      "cell_type": "code",
      "source": [
        "data  = []\n",
        "data.append(clean_text(lower_case))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CJ9CWck9GBWQ"
      },
      "cell_type": "code",
      "source": [
        "# This time we use spacy for lemmatizarion\n",
        "import spacy\n",
        "\n",
        "# Second lemmatization of our data\n",
        "def lemmatization(data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_output = []\n",
        "    for sent in data:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_output\n",
        "\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Lemmatize keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "vBJ-dbJeGBWQ"
      },
      "cell_type": "code",
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "\n",
        "# View\n",
        "print(corpus[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4V74iaINGBWQ"
      },
      "cell_type": "markdown",
      "source": [
        "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of word id and word frequency in the document, i.e. [0,1] word_id 0 means the word id appers first in the document and word frequency it appears once in the document. For Human readable formart see `cell [87-89]` or else run the following code `[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]`.  "
      ]
    },
    {
      "metadata": {
        "id": "hkNDnwXdGBWQ"
      },
      "cell_type": "markdown",
      "source": [
        "### 9.1 Building the topic model with LDA\n",
        "\n",
        "\n",
        "Apart from that, `alpha` and `beta` are hyperparameters that affect sparsity of the topics. Both with default values to 1.0/num_topics prior.\n",
        "\n",
        "`alpha` is the per-document topic distribution, in simple terms it is a matrix where each row is a document and each column is a topic.\n",
        "`beta` is the per-topic word distribution, also in simple terms it is a matrix where each row represents a topic and each column represents a word.\n",
        "\n",
        "\n",
        "Also see online documentation for `gensim.gensim.models.ldamodel.LdaModel()` parameters"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ImCL23zRGBWR"
      },
      "cell_type": "code",
      "source": [
        "# LDA model\n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, alpha='auto', num_topics=20, random_state=100,\n",
        "                                           update_every=1, passes=20, per_word_topics=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "fzqBc6CfGBWR"
      },
      "cell_type": "code",
      "source": [
        "# Lets view the topics in our model\n",
        "print(lda_model.print_topics())\n",
        "doc_lda  = lda_model[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i6erAAwIGBWR"
      },
      "cell_type": "markdown",
      "source": [
        "How do we make sense of this?\n",
        "\n",
        "For topic `0` which is intrepreted as `'0.001*\"need\" + 0.001*\"people\" + 0.001*\"namibian\" + 0.001*\"government\" + 0.001*\"citizen\" + 0.001*\"youth\" + 0.001*\"benefit\" + 0.001*\"system\" + 0.001*\"provide\" + 0.001*\"make\"'`\n",
        "\n",
        "**What does it mean?**  it means that the top 10 keywords that are part of this topic are: `need`, `people`, `namibian`, `government`, `citizen`, `youth`, `benefit`, `system`, `provide` and `make`. The numbers before the words represent the weight of the specific word on that topic, example `need` in topic `0` weigh `0.001` and this is much for all the words top 10 words in topic `0`.\n",
        "\n",
        "\n",
        "Can we deduced what topic this could be by looking on the top 10 keyswords? like what topic could trigger one to talk about `need`, `people`, and so on? We can perhaps summarize it to **POLITICS - Namibian People**\n",
        "\n",
        "This can be done for all the remaining topics to see wether we can come up with a close judgement of each topic.\n",
        "\n",
        "\n",
        "![titile](https://www.machinelearningplus.com/wp-content/uploads/2018/03/Inferring-Topic-from-Keywords.png)"
      ]
    },
    {
      "metadata": {
        "id": "pBSgQyLmGBWR"
      },
      "cell_type": "markdown",
      "source": [
        "### 9.2 Model Perplexity and Coherence Score\n",
        "\n",
        "\n",
        "Lets evaluate our model by computing the perplexity and topic coherence. Before that we need to understang what model perplexity and coherence is.\n",
        "**Perplexity** is an evaluation  metric on how probable (predictive likelihood) new unseen data is given the model that was learned earlier. **Topic coherence** measures a score on a single topic through measuring the degree of semantic similarity of all the high scoring words in a topic. Both model perplexity and topic score provide a convenient measure to judge how good a given topic model is.  "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vFUckYcPGBWR"
      },
      "cell_type": "code",
      "source": [
        "# Print model perplexity\n",
        "print('\\nPerplexity:', lda_model.log_perplexity(corpus))\n",
        "\n",
        "\n",
        "# Coherence Score\n",
        "\n",
        "coherence_model_lda = CoherenceModel(lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score:', coherence_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o2Meg-KDGBWR"
      },
      "cell_type": "markdown",
      "source": [
        "* The coherance score is 0.27 and perpelexity is -7.88"
      ]
    },
    {
      "metadata": {
        "id": "UVOOaAQ4GBWR"
      },
      "cell_type": "markdown",
      "source": [
        "### 9.3 Visualize topic's keywords"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3mUjPSVNGBWR"
      },
      "cell_type": "code",
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis_topics = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EZh4ZFzKGBWS"
      },
      "cell_type": "code",
      "source": [
        "vis_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gvXhMgu0GBWS"
      },
      "cell_type": "markdown",
      "source": [
        "What can we deduce from graph output?\n",
        "\n",
        "On the left it shows a buble in which it represnt a topic, also note we are supposed to have 20 bubles since we chose to have 20 topics in our model earlier on. The larger the buble, the more dominant that topic is.\n",
        "\n",
        "\n",
        "To determine a good topic, the bubble on the left would be big and non-overlapping throught the chart and being in spread along all the quadrants in stead of being clustered. Also note we only have one big bubble which means that we classified our topic 1 very well, however if you look closely you will see a black dot which seem to have all the remaining topic clustered together and overlaping each other in one place.\n",
        "\n",
        "\n",
        "\n",
        "If you move the cursor on the bubble, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic, i.e, top 30 most relevant words. Clicking the  `Previous Topic` and `Next Topic`  buttons on the upper left, we again see the words on the right-hand side updating. However, there isn't much significant changes in most of the words in all the topics as they are all dominated by words like `need`, `youth`, `people`, `government`, `country` , `namibian` and so on... We can say\n",
        "\n",
        "With the output, we can say that the topics in our document exacerbate around **politics**, **Namibian politics** to be specific. Thus we have successfully build a good looking topic model."
      ]
    },
    {
      "metadata": {
        "id": "ImTtq1HAGBWS"
      },
      "cell_type": "markdown",
      "source": [
        "### **10. Conclusion**\n",
        "\n",
        "\n",
        "* We have succefully analysed the document eventhough there are many models that can be use to throughly get an idea of the whole document.\n",
        "\n",
        "* The data cleaning process had issues in which there we characters that were not english and thus have tempered with our analysis, this can be improved.\n",
        "\n",
        "* Getting the top 100 most common words resonate well with me on a personal level looking at the state in which Namibia is and what that is needed to be done. This could be because of the word **youth** since I happen fall under that group, haha :-D\n",
        "\n",
        "* For sentiment analysis, we found the document to be neutral with almost factual information in the document rather than public opinions, beliefs and so forth.\n",
        "\n",
        "* The summarization of the document was quite challenging using the TextRank algorimth together with the implementaion of the similarity matrix, however it was easily done with the `gensim` package in one line of code, CAN YOU IMAGINE? Me neither :-;\n",
        "\n",
        "* Topic modelling  was successful, in which we could say that the document is focused on Namibian politics.\n",
        "\n",
        "**Hope you enjoyed reading this. I would appreciate if you leave your thoughts in the comments section. And dont forget to upvote if you liked it**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "TS_KGzyJGBWS",
        "outputId": "cee5e70f-e3e9-4dfb-ea98-472285b8cb53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-f08d119f1b6c>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f08d119f1b6c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ipython nbconvert notebook.ipynb --to script\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NLP analysis of PDF documents",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}